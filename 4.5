what is checksum and the importance of checksum and how hadoop performs checksum:
Checksums are used to ensure the integrity of a file after it has been transmitted from one storage device to another. This can be across
the Internet or simply between two computers on the same network. Either way, if you want to ensure The checksum is calculated using a
hash function and is normally posted along with the download. To verify the integrity of 
the file, a user calculates the checksum using a checksum calculator program and then compares the two to make sure they match,
that the transmitted file is exactly the same as the source file, you can use a checksum.
Checksums are used not only to ensure a corrupt-free transmission, but also to ensure that the file has not been tampered with.
When a good checksum algorithm is used, even a tiny change to the file will result in a completely different checksum value.
How hadoop perform checksum value:-
The Hadoop LocalFileSystem performs client-side checksumming.
client used to calculate the checksum for each chunk of data and pass it to datanodes for storage.This means that when you write a file called filename, the filesystem client transparently
creates a hidden file, .filename.crc, in the same directory containing the checksums for each chunk of the file.
Checksums are verified when the file is read, and if an error is detected, LocalFileSystem throws a ChecksumException.

Anatomy of file write to HDFS:
Client creates, calls create() on Distributed File System.Distributed File System contacts name node to create a new file in the file system’s namespace, with no blocks associated with it.
The name node performs various checks to make sure the file doesn’t already exist and that the client has the right permissions to create
the file. If these checks pass, the name node makes a record of the new file.Distributed File System returns an FSDataOutputStream for the client to start writing data. FSDataOutputStream uses DFSOutputStream,which handles communication with the data nodes and name node.Client signals write() method on FSDataOutputStream.
DFSOutputStream splits data into packets and writes it to an internal queue called the data queue.
When the client has finished writing data, it calls close() on the stream. This flushes all the 
remaining packets to the data node pipeline and waits for acknowledgments.
The name node already knows which blocks the file is made up of ((because DataStreamer had asked for block allocations)), 
so it only has to wait for blocks to be minimally replicated before returning successfully.
Closing a file in HDFS performs an implicit hflush().
After a successful return from hflush(), HDFS guarantees that the data written up to that point in
the file has reached all the Data nodes in the write pipeline and is visible to all new readers.
 	
How HDFS handles failures during file write.
The pipeline is closed and any packets in the ackqueue are added to the front of the data queue.The current block on the good data nodes is given a new identity, which is communicated to the name node
The failed data node is removed from the pipeline, and a new pipeline is constructed from the two good datanodes.The remainder of the block’s data is written to the good data nodes in the pipeline.
The name node notices that the block is under-replicated, and it arranges for a further replica to be created on another node. As long as dfs.namenode.replication.minreplicas (which defaults to 1) are written, the write will succeed.
